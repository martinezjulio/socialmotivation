{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from agents import banditagents\n",
    "from environments import bandits\n",
    "import utils\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dir = '/Users/juliomartinez/Documents/PhD/socialmotivation/configs'\n",
    "num_episodes = 500\n",
    "\n",
    "solvers = ['GreedyAgent', 'EpsilonGreedyAgent', 'UCBAgent']\n",
    "#solvers = ['UCBAgent']\n",
    "environment = 'TwoArmBandit'\n",
    "\n",
    "bandit_config_filename = os.path.join(configs_dir, environment + '.yaml')\n",
    "bandit_config = utils.get_config(bandit_config_filename)\n",
    "env = bandits.Bandit(bandit_config)\n",
    "num_iterations = 1000\n",
    "\n",
    "results_column_names = ['episode','iteration','solver','reward_demonstrator','reward_learner','chosen_arm_id_demonstrator','chosen_arm_id_learner']\n",
    "#results_column_name = ['episode', 'iteration', 'solver', 'agent', 'reward', 'best_arm_id']\n",
    "#best_arm_dist_column_names = ['solver', 'num_episodes', 'num_iterations', 'prob_arm1', 'prob_arm2']\n",
    "\n",
    "\n",
    "for i, solver in enumerate(solvers):\n",
    " \n",
    "    # Get solver class \n",
    "    agentClass = getattr(banditagents, solver)\n",
    "    \n",
    "    # Get config file\n",
    "    agents_config_filename = os.path.join(configs_dir, solver + '.yaml')\n",
    "    agents_config = utils.get_config(agents_config_filename)\n",
    "\n",
    "    demonstrator_best_arm_distribution = np.zeros(env.num_arms)\n",
    "    learner_best_arm_distribution = np.zeros(env.num_arms)\n",
    "\n",
    "    for episode_j in range(num_episodes):\n",
    "        # run demonstrator\n",
    "        demonstrator = agentClass(agents_config['demonstrator'])   \n",
    "        demonstrator(env)\n",
    "        \n",
    "        # run learner\n",
    "        learner = agentClass(agents_config['learner'])\n",
    "        learner(env, demonstrator)\n",
    "\n",
    "        # store results\n",
    "        demonstrator_best_arm_distribution[demonstrator.best_arm_id]=+1\n",
    "        learner_best_arm_distribution[learner.best_arm_id]=+1\n",
    "\n",
    "        num_iterations = len(demonstrator.reward_history)\n",
    "        iterations = list(range(num_iterations))\n",
    "        episodes = [episode_j]*num_iterations\n",
    "        solvers_ = [solver]*num_iterations\n",
    "        trial_df = pd.DataFrame(list(zip(episodes, iterations, solvers_, demonstrator.reward_history,learner.reward_history, demonstrator.arm_id_history, learner.arm_id_history)), columns=results_column_names)\n",
    "        if i < 1 and episode_j < 1:\n",
    "            results_df = trial_df.copy()\n",
    "        else:\n",
    "            results_df = pd.concat([results_df,trial_df],join='inner', ignore_index=True)\n",
    "\n",
    "    demonstrator_best_arm_distribution = demonstrator_best_arm_distribution / np.sum(demonstrator_best_arm_distribution)\n",
    "    learner_best_arm_distribution = learner_best_arm_distribution / np.sum(learner_best_arm_distribution)\n",
    "    solver_df = pd.DataFrame({\n",
    "        'solver':solver, \n",
    "        'num_episodes': num_episodes, \n",
    "        'num_iterations': num_iterations, \n",
    "        'demonstrator_prob_arm0':demonstrator_best_arm_distribution[0],\n",
    "        'demonstrator_prob_arm1':demonstrator_best_arm_distribution[1], \n",
    "        'learner_prob_arm0':learner_best_arm_distribution[0],\n",
    "        'learner_prob_arm1':learner_best_arm_distribution[1]\n",
    "        }, index=[0])\n",
    "    if i < 1:\n",
    "        arm_dist_df = solver_df.copy()\n",
    "    else:\n",
    "        arm_dist_df = pd.concat([arm_dist_df,solver_df],axis=0)\n",
    "\n",
    "results_df.to_csv('/Users/juliomartinez/Documents/PhD/socialmotivation/results.csv')\n",
    "display(HTML(results_df.head().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data and Compute Helper Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cumulative reward (over iterations) to results_df\n",
    "results_df = pd.read_csv('/Users/juliomartinez/Documents/PhD/socialmotivation/results.csv', index_col=0).sort_values(by=['solver', 'episode', 'iteration'], ignore_index=True)\n",
    "results_df['cumulative_reward_demonstrator'] = results_df.groupby(['solver','episode'])['reward_demonstrator'].cumsum()\n",
    "results_df['cumulative_reward_learner'] = results_df.groupby(['solver','episode'])['reward_learner'].cumsum()\n",
    "\n",
    "# Compute average cumulative reward (average across episodes)\n",
    "avg_cumulative_reward_demonstrator = pd.Series(results_df.groupby(['solver','iteration'])['cumulative_reward_demonstrator'].mean(), name='avg_cumulative_reward_demonstrator')\n",
    "avg_cumulative_reward_learner = pd.Series(results_df.groupby(['solver','iteration'])['cumulative_reward_learner'].mean(), name='avg_cumulative_reward_learner')\n",
    "avg_cumulative_reward_df = pd.concat([avg_cumulative_reward_demonstrator,avg_cumulative_reward_learner],axis=1)\n",
    "avg_cumulative_reward_df['delta_of_avg_cumulative_reward'] = avg_cumulative_reward_df['avg_cumulative_reward_learner'].sub(avg_cumulative_reward_df['avg_cumulative_reward_demonstrator'], axis = 0)\n",
    "avg_cumulative_reward_df = avg_cumulative_reward_df.reset_index().sort_values(by=['solver', 'iteration'], ignore_index=True)\n",
    "\n",
    "# Differences in cumulative reward for each iteration\n",
    "delta_df = results_df[['episode', 'iteration', 'solver']]\n",
    "delta_df['delta_of_cumulative_reward'] = results_df['cumulative_reward_learner'].sub(results_df['cumulative_reward_demonstrator'], axis = 0)\n",
    "delta_df = delta_df.sort_values(by=['solver', 'episode', 'iteration'], ignore_index=True)\n",
    "\n",
    "# change to wide format\n",
    "results_long_df = pd.wide_to_long(\n",
    "    results_df, \n",
    "    stubnames=['reward', 'chosen_arm_id', 'cumulative_reward'], \n",
    "    i=['episode', 'iteration', 'solver'], \n",
    "    j='agent',\n",
    "    sep='_', \n",
    "    suffix=r'\\w+').reset_index().sort_values(by=['solver', 'episode', 'iteration'], ignore_index=True)\n",
    "print('Results Pivot Longer')\n",
    "display(HTML(results_long_df.head().to_html()))\n",
    "\n",
    "\n",
    "print('\\n\\nResults')\n",
    "display(HTML(results_df.head().to_html()))\n",
    "\n",
    "print('\\n\\nAverge Cumulative Reward')\n",
    "display(HTML(avg_cumulative_reward_df.head().to_html()))\n",
    "\n",
    "print('\\n\\nDelta of Each Iteration')\n",
    "display(HTML(delta_df.head().to_html()))\n",
    "\n",
    "print('\\n\\nResults in Wide Format')\n",
    "display(HTML(results_long_df.head().to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "nrows = 1\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6*ncols,6))\n",
    "sns.lineplot(data=results_long_df, x=\"iteration\", y=\"cumulative_reward\", hue=\"solver\", style=\"agent\", ax=axes[0])\n",
    "axes[0].set_ylabel('Average Cumulative Reward')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].axvline(x=100, ls=':', lw=2, label='number of initial iterations', color='grey')\n",
    "\n",
    "#sns.lineplot(data=avg_cumulative_reward_df, x=\"iteration\", y=\"delta_of_avg_cumulative_reward\", hue=\"solver\", ax=axes[1])\n",
    "#axes[1].set_ylabel('$\\Delta$(Average(Cumulative Reward))')\n",
    "#axes[1].set_xlabel('Iteration')\n",
    "#axes[1].axvline(x=100, ls=':', lw=2, label='number of initial iterations', color='grey')\n",
    "\n",
    "sns.lineplot(data=delta_df, x=\"iteration\", y=\"delta_of_cumulative_reward\", hue=\"solver\", ax=axes[1])\n",
    "axes[1].set_ylabel('Avg( $\\Delta$(Cumulative Reward) )')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].axvline(x=100, ls=':', lw=2, label='number of initial iterations', color='grey')\n",
    "#plt.suptitle('Observe Current Iter')\n",
    "#plt.suptitle('Observe Best Arm ($p_{exclude}=0$) for Greedy and Epsilon Greedy')\n",
    "#plt.suptitle('Observe Best Arm ($p_{exclude}=1$) for Greedy and Epsilon Greedy')\n",
    "#plt.suptitle('Observe Best Arm ($p_{exclude}=0.5$) for Greedy and Epsilon Greedy')\n",
    "#plt.suptitle('Observe Simulatenously and Current Iter')\n",
    "#plt.suptitle('Observe Best Arm ($p_{exclude}=0$) and Observe Simultaneously')\n",
    "plt.suptitle('Observe Best Arm ($p_{exclude}=1$) and Observe Simultaneously')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6d25498612cb4f5f159617db40943c8f1e1957d59ad3740488367ba41964ca3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 ('socialMot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
