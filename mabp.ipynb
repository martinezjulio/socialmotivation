{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from agents import banditagents\n",
    "from environments import bandits\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'observation_simultaneous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_solvers):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 22\u001b[0m         agent \u001b[38;5;241m=\u001b[39m \u001b[43mbanditagents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGreedyAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         env \u001b[38;5;241m=\u001b[39m bandits\u001b[38;5;241m.\u001b[39mBandit(bandit_config)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m#total_reward = 0 # to store episode return\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/socialmotivation/agents/banditagents.py:80\u001b[0m, in \u001b[0;36mGreedyAgent.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=78'>79</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m---> <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=79'>80</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config)\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=80'>81</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_agent\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mrandom_agent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/PhD/socialmotivation/agents/banditagents.py:13\u001b[0m, in \u001b[0;36mBaseBanditAgent.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_solver \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39magent_solver\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocial_agent \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39msocial_agent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserve_simultaenously \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39mobservation_simultaneous\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m# True or False (if False choice alternative)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserve_action_only \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mobservation_action_only\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# True or False (if False action and reward)\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/juliomartinez/Documents/PhD/socialmotivation/agents/banditagents.py?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserve_current_iteration \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mobservation_current_iteration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# 'current', 'best'\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'observation_simultaneous'"
     ]
    }
   ],
   "source": [
    "bandit_config = {}\n",
    "bandit_config['payoffs'] = [0.6, 0.4]\n",
    "bandit_config['sampling_distribution'] = 'uniform'\n",
    "\n",
    "agent_config = {}\n",
    "agent_config['num_iterations'] = 2000 # episode length\n",
    "agent_config['num_initial_rounds'] = 10\n",
    "agent_config['decay'] = None\n",
    "agent_config['epsilon'] = None\n",
    "agent_config['optimistic'] = None\n",
    "agent_config['solver'] = 'greedy'\n",
    "agent_config['random_agent'] = False\n",
    "agent_config['social_agent'] = False\n",
    "\n",
    "num_episodes = 50\n",
    "solvers = ['greedy']\n",
    "all_rewards = np.empty(shape=(len(agent_solvers),num_episodes, agent1_config['num_iterations']),dtype=float)\n",
    "all_actions = np.empty(shape=(len(agent_solvers),num_episodes, agent1_config['num_iterations']),dtype=float)\n",
    "\n",
    "for i, solver in enumerate(solvers):\n",
    "    for j in range(num_episodes):\n",
    "        agent = banditagents.GreedyAgent(agent1_config)\n",
    "        env = bandits.Bandit(bandit_config)\n",
    "        #total_reward = 0 # to store episode return\n",
    "\n",
    "        actions, rewards = agent(env)\n",
    "        all_rewards[i,j,:] = rewards\n",
    "        all_actions[i,j,:] = actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_per_iteration = np.mean(all_rewards, axis=1)\n",
    "sd_reward_per_iteration = np.mean(all_rewards, axis=1)\n",
    "\n",
    "mean_cumulative_reward = np.cumsum(mean_reward_per_iteration,axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(12,4))\n",
    "\n",
    "for i, agent_type in enumerate(agent_types):\n",
    "    axes[1].plot(mean_cumulative_reward[i], label=agent_type)\n",
    "    axes[0].plot(mean_reward_per_iteration[i], label=agent_type)\n",
    "\n",
    "axes[1].set_ylabel('Average Cumulative Reward')\n",
    "axes[0].set_ylabel('Average Reward')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "#axes[0].set_title('Cumulative Rewards')\n",
    "#axes[1].set_title('Average Rewards')\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e5e66ecaa405548d2fdd31030707dea014102c53f7c3e0d0c394e67cebb384f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
